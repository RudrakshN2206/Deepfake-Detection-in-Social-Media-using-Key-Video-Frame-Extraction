{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv-prCVTFmhW",
        "outputId": "2a393560-3f9e-4a6f-b7fe-0d9bcb44c21d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQi0P-MSFd1M",
        "outputId": "2ac6db62-aa51-4354-8a7b-fe032b39d501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No face detected in temp_keyframes/frame_002.png\n",
            "No face detected in temp_keyframes/frame_006.png\n",
            "No face detected in temp_keyframes/frame_005.png\n",
            "No face detected in temp_keyframes/frame_001.png\n",
            "No face detected in temp_keyframes/frame_002.png\n",
            "No face detected in temp_keyframes/frame_005.png\n",
            "No face detected in temp_keyframes/frame_008.png\n",
            "No face detected in temp_keyframes/frame_007.png\n",
            "No face detected in temp_keyframes/frame_003.png\n",
            "No face detected in temp_keyframes/frame_004.png\n",
            "No face detected in temp_keyframes/frame_005.png\n",
            "No face detected in temp_keyframes/frame_008.png\n",
            "Found 707 images belonging to 2 classes.\n",
            "Found 220 validated image filenames belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - accuracy: 0.6217 - loss: 0.7168 - val_accuracy: 0.6909 - val_loss: 0.5574\n",
            "Epoch 2/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 853ms/step - accuracy: 0.7768 - loss: 0.4798 - val_accuracy: 0.7182 - val_loss: 0.5282\n",
            "Epoch 3/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 803ms/step - accuracy: 0.8139 - loss: 0.3981 - val_accuracy: 0.7364 - val_loss: 0.5656\n",
            "Epoch 4/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 841ms/step - accuracy: 0.8355 - loss: 0.3679 - val_accuracy: 0.7364 - val_loss: 0.5703\n",
            "Epoch 5/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 811ms/step - accuracy: 0.8589 - loss: 0.2948 - val_accuracy: 0.7500 - val_loss: 0.6104\n",
            "Epoch 6/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 810ms/step - accuracy: 0.8471 - loss: 0.3437 - val_accuracy: 0.7409 - val_loss: 0.5924\n",
            "Epoch 7/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 823ms/step - accuracy: 0.8967 - loss: 0.2653 - val_accuracy: 0.7500 - val_loss: 0.5635\n",
            "Epoch 8/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 806ms/step - accuracy: 0.8521 - loss: 0.3040 - val_accuracy: 0.7318 - val_loss: 0.6352\n",
            "Epoch 9/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 861ms/step - accuracy: 0.8607 - loss: 0.3181 - val_accuracy: 0.7455 - val_loss: 0.5688\n",
            "Epoch 10/10\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 808ms/step - accuracy: 0.8886 - loss: 0.2784 - val_accuracy: 0.7409 - val_loss: 0.6113\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 677ms/step\n",
            "Frame-level accuracy: 0.7409\n",
            "Video-level accuracy: 0.8462\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import cv2\n",
        "from mtcnn import MTCNN\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "def extract_key_frames(video_path, temp_dir):\n",
        "    if not os.path.exists(temp_dir):\n",
        "        os.makedirs(temp_dir)\n",
        "    cmd = [\n",
        "        'ffmpeg',\n",
        "        '-i', video_path,\n",
        "        '-vf', \"select='eq(pict_type\\\\,I)'\",\n",
        "        '-vsync', 'vfr',\n",
        "        os.path.join(temp_dir, 'frame_%03d.png')\n",
        "    ]\n",
        "    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "def detect_and_crop_face(image_path, output_path):\n",
        "    detector = MTCNN()\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Failed to load image: {image_path}\")\n",
        "        return\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    faces = detector.detect_faces(image_rgb)\n",
        "    if len(faces) > 0:\n",
        "        faces = sorted(faces, key=lambda x: x['box'][2] * x['box'][3], reverse=True)\n",
        "        box = faces[0]['box']\n",
        "        x, y, w, h = box\n",
        "        padding = 20\n",
        "        x1 = max(0, x - padding)\n",
        "        y1 = max(0, y - padding)\n",
        "        x2 = min(image.shape[1], x + w + padding)\n",
        "        y2 = min(image.shape[0], y + h + padding)\n",
        "        cropped = image[y1:y2, x1:x2]\n",
        "        cropped_resized = cv2.resize(cropped, (299, 299))\n",
        "        cv2.imwrite(output_path, cropped_resized)\n",
        "    else:\n",
        "        print(f\"No face detected in {image_path}\")\n",
        "\n",
        "train_videos = list(range(1, 41))\n",
        "val_videos = list(range(41, 54))\n",
        "\n",
        "for split, videos in [('train', train_videos), ('val', val_videos)]:\n",
        "    for label in ['real', 'fake']:\n",
        "        output_dir = f'processed/{split}/{label}'\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "        for video_num in videos:\n",
        "            if label == 'real':\n",
        "                video_path = f'/content/drive/My Drive/SDFVD/videos_real/v{video_num}.mp4'\n",
        "            else:\n",
        "                video_path = f'/content/drive/My Drive/SDFVD/videos_fake/vs{video_num}.mp4'\n",
        "            temp_dir = 'temp_keyframes'\n",
        "            extract_key_frames(video_path, temp_dir)\n",
        "            for frame_file in os.listdir(temp_dir):\n",
        "                frame_path = os.path.join(temp_dir, frame_file)\n",
        "                output_frame_path = os.path.join(output_dir, f'video{video_num}_{frame_file}')\n",
        "                detect_and_crop_face(frame_path, output_frame_path)\n",
        "            for file in os.listdir(temp_dir):\n",
        "                os.remove(os.path.join(temp_dir, file))\n",
        "            if os.path.exists(temp_dir):\n",
        "                os.rmdir(temp_dir)\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2\n",
        ")\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'processed/train',\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_real_frames = [f for f in os.listdir('processed/val/real') if os.path.isfile(os.path.join('processed/val/real', f))]\n",
        "val_fake_frames = [f for f in os.listdir('processed/val/fake') if os.path.isfile(os.path.join('processed/val/fake', f))]\n",
        "val_df = pd.DataFrame({\n",
        "    'filename': [os.path.join('processed/val/real', f) for f in val_real_frames] +\n",
        "                [os.path.join('processed/val/fake', f) for f in val_fake_frames],\n",
        "    'class': ['real'] * len(val_real_frames) + ['fake'] * len(val_fake_frames),\n",
        "    'video_id': [f'video{f.split(\"_\")[0].replace(\"video\", \"\")}_{\"real\" if \"real\" in f else \"fake\"}' for f in val_real_frames + val_fake_frames]\n",
        "})\n",
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "    val_df,\n",
        "    x_col='filename',\n",
        "    y_col='class',\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "base_model = Xception(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dropout(0.5),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "base_model.trainable = False\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_generator, epochs=10, validation_data=val_generator)\n",
        "\n",
        "val_predictions = model.predict(val_generator)\n",
        "val_pred_labels = np.argmax(val_predictions, axis=1)\n",
        "val_true_labels = val_generator.classes\n",
        "\n",
        "frame_accuracy = accuracy_score(val_true_labels, val_pred_labels)\n",
        "print(f'Frame-level accuracy: {frame_accuracy:.4f}')\n",
        "\n",
        "video_predictions = {}\n",
        "for i, video_id in enumerate(val_df['video_id']):\n",
        "    if video_id not in video_predictions:\n",
        "        video_predictions[video_id] = []\n",
        "    video_predictions[video_id].append(val_pred_labels[i])\n",
        "\n",
        "video_pred_labels = {}\n",
        "for video_id, preds in video_predictions.items():\n",
        "    video_pred_labels[video_id] = 1 if 1 in preds else 0\n",
        "\n",
        "video_true_labels = {vid: 0 if vid.endswith('_real') else 1 for vid in video_predictions.keys()}\n",
        "correct = sum(1 for vid, pred in video_pred_labels.items() if pred == video_true_labels[vid])\n",
        "video_accuracy = correct / len(video_pred_labels)\n",
        "print(f'Video-level accuracy: {video_accuracy:.4f}')"
      ]
    }
  ]
}